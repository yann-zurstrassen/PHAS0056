{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Identifying collisions containing Higgs bosons at the Large Hadron Collider\n",
    "\n",
    "One of the primary physics objectives of the Large Hadron Collider is to study the Higgs Boson and understand the process by which particles acquire mass. The Higgs Boson's decays channels along with the respective branching ratios are shown in the figure below. \n",
    "\n",
    "<img src=\"https://physicsforme.files.wordpress.com/2012/07/higgs_decay.jpg\" width=\"400\" />\n",
    "\n",
    "In 2012 the Higgs Boson was <a href=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HIGG-2012-27/\">discovered</a> in the $H\\rightarrow\\gamma\\gamma$ and $H\\rightarrow ZZ \\rightarrow 4l$ decay channels. The plot below shows a clear excess of collisions (also referred to as events) over the background processes at around 125 GeV, when looking at the Higgs boson mass reconstructed using the four leptons, in the $H\\rightarrow ZZ \\rightarrow 4l$ 'golden channel'. \n",
    "\n",
    "<img src=\"https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2017/newatlasprec.png\" width=\"400\" />\n",
    "\n",
    "However, the Higgs Boson predominantly decays to a pair of $b$-quarks ($H\\rightarrow b\\bar{b}$), but it actually took till 2018 for it to be <a href=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/HIGG-2018-04/\"> observed at </a> $5\\sigma$ sensitivity in this decay channel. This is due to the discovery channels leaving a clean signature in the detector that is easy to isolate, making searches in the $H\\rightarrow\\gamma\\gamma$ or $H\\rightarrow ZZ \\rightarrow 4l$ favourable. As explaiend below, $H\\rightarrow b\\bar{b}$ searches have much larger and more complicated backgrounds, which necessiatates the use of complicated machine learning algorithms at many stages of the analysis to be able to isolate the small number of collisions containing $H\\rightarrow b\\bar{b}$ over the overwhelming number of collisions containing only background processes. This challenge will involve training a supervised classification Neural Network to identify collisions containing $H\\rightarrow b\\bar{b}$ using ATLAS simulated collision data, with the aim of maximising ATLAS's sensitivity to this vital Higgs decay channel.\n",
    "\n",
    "## Large background \n",
    "\n",
    "Firstly, the $H\\rightarrow b\\bar{b}$ process at the LHC has enourmous amount of background with final states that are the same as the signal i.e. two $b$-quarks ($b\\bar{b}$). The figure below shows the cross sections (the likelihood of producing a given process) for a Higgs boson compared to $b\\bar{b}$ at the LHC. In the LHC current operational energy, only around 1 in a trillion proton-proton collisions create a Higgs boson. \n",
    "\n",
    "<img src=\"images/lhc_cross_sections.png\" width=\"400\" />\n",
    "\n",
    "Hence, complicated techniques need to be deployed to identify the small amount of signal over the background processes that are several magnitudes larger. \n",
    "\n",
    "## Jet Production\n",
    "\n",
    "Secondly, precisely measuring and distinguishing $b$-quarks from say $c$-quarks or light-flavour quarks is also challenging. When a pair of $b$-quarks is created in the ATLAS detector they go through a process known as hadronisation and form [jets](https://www.youtube.com/watch?v=FMH3T05G\\_to). The jets must be identified as originating from b-quarks (b-jets) by a process known as <a href=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/FTAG-2018-01/\">b-tagging</a>. This extra level of complexity along with the large background makes searches for $H\\rightarrow b\\bar{b}$ challenging. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The WH 1-Lepton Channel\n",
    "\n",
    "The process we will be searching for is shown in the Feynman Diagram below. A Higgs Boson is radiated off a $W^\\pm$ boson which subsequently decays to a pair of _b_-quarks. The $W^\\pm$ boson then goes on to decay into a lepton and a corresponding neutrino. \n",
    "\n",
    "<img src=\"images/one-lepton.png\" width=\"350\" />\n",
    "\n",
    "\n",
    "The final state products of a 1-lepton channel $H\\rightarrow b\\bar{b}$ process are:\n",
    "   * A Neutrino [characterised as missing transverse energy in the detector].\n",
    "   * A charged Lepton (e u) [characterised by the transverse momentum and direction].\n",
    "   * 2 _b_-jets [characterised by their transverse momentum, direction, distance between them and their reconstructed mass].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Separating Signal from Background\n",
    "\n",
    "\n",
    "We identify what particles were created in collisions using kinematic and topological variables, which describe the properties of the objects reconstructed by the ATLAS detector. The signal and background processes all have a distinct signature based upon the underlying production and decay of the process, which can be identified from studying these variables. A list of the variables that can be used in this exercise is shown below: \n",
    "\n",
    "\n",
    "\n",
    "| Variable        | Description           | Label  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "|$n_J$                   | Number of jet in the event (this is always 2 in this exercise) | nJ |\n",
    "|$n_{\\text{Tags}}$       | Number of b-tagged jets in the event (this is always two in this exercise) | nTags |\n",
    "|$\\Delta R(b_1b_2)$      | Angular distance between the two *b*-tagged jets | dRBB |\n",
    "| $p_T^B1$                | Reconstructed transverse momentum of the b-tagged jet with the highest $p_{T}$                      | pTB1 |\n",
    "| $p_T^B2$                | Reconstructed transverse momentum of the b-tagged jet with the 2nd highest $p_{T}$                      | pTB2 |\n",
    "| $p_T^V$                | Reconstructed transverse momentum of the vector boson                      | pTV |\n",
    "| $m_{BB}$               | Reconstructed mass of the Higgs boson from the two b-tagged jets                     | mBB |\n",
    "| $m_{top}$              | Reconstructed top quark mass                     | Mtop |\n",
    "| $m_{T}^{W}$              | Reconstructed transverse mass of the W boson                     | mTW |\n",
    "| $E^{Miss}_{T}$         | Missing transverse energy                        | MET |\n",
    "| $m^{W}_{T}$            | Reconstructed transverse mass of the W boson                        | mTW |\n",
    "| $dY(W, H)$             | Separation between the W boson and the Higgs candidate                        | dYWH |\n",
    "| $d\\phi(W, H)$          | Angular seperationg in the transverse plane between the W boson and the Higgs candidate                        | dPhiVBB |\n",
    "| $MV1^{B1}_\\text{cont}$        | The classification output of whether the leading jet is a b or not (the higher the value the more likely it is a b-jet) | MV1cB1_cont |\n",
    "| $MV1^{B2}_{\\text{cont}}$        | The classification output of whether the sub-leading jet is a b or not (the higher the value the more likely it is a b-jet) | MV1cB2_cont |\n",
    "| $n^{\\text{Jets}}_{\\text{cont}}$        | Number of additional jets found in the event | nTrackJetsOR |\n",
    "\n",
    "\n",
    "                    Table 1: Kinematic and topological paramaeters used to identify events. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sensitivity\n",
    "\n",
    "The key metric that will be used to determine how well the model performs, is called the _signal sensitivity_, which determines given the number of signal events compared to the number of background events, the likelihood that you will see the process of interest. Functions that will calculate the _signal sensitivity_ are provided, but if you want to read more about how this is calculated please refer to the profile likelihood ratio test (1(b) on p23) in this <a href=\"https://www.pp.rhul.ac.uk/~cowan/stat/cowan_munich16.pdf\">talk</a>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tasks\n",
    "\n",
    "Baseline\n",
    "- Produce an optimised cut-based analysis using the di-jet mass as a discriminant to use as baseline and for comparison (shouldn’t spend more than 1-2 days on this). A notebook that reads in the data, visualises the various distributions and calculates the signal sensitivity is provided as a starting point here:\n",
    "\t+ https://github.com/nikitapond/in2HEP/blob/master/notebooks/ATLAS%20Cut%20Based.ipynb\n",
    "- Produce a simple optimised NN-based supervised classifier to seperate signal vs background. A notebook that reads in the data, draws the classifier output and calculates the _signal sensitivity_ is provided as a starting point here: \n",
    "\t+ https://github.com/nikitapond/in2HEP/blob/master/notebooks/ATLAS_NN.ipynb\n",
    "- Determine what the optimal settings are for the following parameters:\n",
    "\t+ Pre-processing of input variables\n",
    "\t+ Number of nodes in layers\n",
    "\t+ Number of hidden layers\n",
    "\t+ Training parameters\n",
    "\t+ Activation functions\n",
    "\t+ Optimisation algorithms\n",
    "\t+ Loss function\n",
    "- Determine:\n",
    "\t+ Improvement over a cut-based approach\n",
    "\t+ If we have enough training statistics (vary number of input events used for training separately for signal and background from 0 -> 100%, does it plateau?)\n",
    "\t+ The importance of the input variables (remove one variable at a time and retrain, how much does the sensitivity degrade by?).\n",
    "\n",
    "Possible extensions:\n",
    "- Investigate which events are selected in the most sensitive region (the high NN output region), are these similar to those selected in the cut-based approach?\n",
    "- Determine training uncertainty (how much does performance vary when re-training with an identical configuration a large number of times).\n",
    "- What happens to the performance if we add some distortion to the most important distribution (to replicate what would happen if our simulation is not an accurate depiction of the collision data)?\n",
    "- What is the best way to determine the hyper-parameter optimisation Baysien, stochastic or a grid search?\n",
    "- Train a multi-classifier or MVA cascade for the different backgrounds (V+bb, tt, diboson), how does this compare \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Frequently asked questions\n",
    "\n",
    "- What is the point of the cut-based analysis?\n",
    "\n",
    "Whenever implementing a machine learning based solution, it is always excellent practice, if possible, to first implement a simple non-ML solution to the problem. This will provide you with an easy-to-understand baseline model against which to judge the (black box) ML solution. This allows you to judge how much improvement your NN brings, to judge if this is reasonable and to compare what the NN is doing against a fully understood model.\n",
    "\n",
    "In the case of this task, the NN is effectively placing cuts on the events to divide them into regions with different signal to background ratios (the more signal like on the right and the more background on the left). One of the suggested tasks, is to study the selection the NN is placing on the events in the rightmost bin, by identifying these events and drawing their input variables distributions. These can then be compared to the cut-based selection you optimised, providing some insight into what the NN is actually doing rather than it being a complete black-box.\n",
    "\n",
    "- Can I use an MVA for the cut-based analysis?\n",
    "\n",
    "No, as described in the previous reply, the cut-based analysis should be simple and transparent, so it is easy to understand. The selection should therefore be simple, transparent and understandable, in terms of both what the selection is, how it was derived and what the impact is. However, to select the optimal cut-values, it is possible to us some simple automated functions (such as grid searches) or to quickly write your own algorithm to perform this function. You really shouldn’t spend more than 1-2 days on producing an optimised cut-based analysis and only place relatively simple cuts on 4-5 variables though.\n",
    "\n",
    "- How should I optimise the cut-based analysis?\n",
    "\n",
    "Each event can be described by numerous variables, which describe the relative kinematics of the particles created during the collision (the electron/muon, neutrino (which appears as missing energy) and the two b-jets). If you examine what these variables look like for the main background processes (tt, W+jets and single-top) and the signal process (VH), it can be observed that depending on the underlying process, these variables look different. As such, it is possible to place a selection on a variable which provides good separation between the signal and background, removing all the events which fail this cut and keeping the subset of events which pass the cut. In the example in the notepad, we place a cut on m_top, removing all the events with m_top < 250 GeV and keeping the subset above this cut-value, plotting the mBB distribution before this cut and the mBB distribution for the subset of events which pass the selection (better sensitivity).\n",
    "\n",
    "- What do the histograms show?\n",
    "\n",
    "The histograms show our best prediction using simulated samples, of the different processes we would generate/collect using the ATLAS detector and the selection we are applying (the electron/muon, neutrino (which appears as missing energy) and the two b-jets). As well as the signal process (VH), there are several backgrounds (the most important being the production of a pair of top-quarks [tt], the production of a single top-quark [Single top] and the production of a W-boson with b-jets [W+(bb,bc,cc,bl)]), which can produce the same selection of final state particles. The simulated events predict how many events of each process we can expect to see overall and will randomly predict the kinematics of the particles produced (and therefore the values for each of the input variables). Each histogram has several bins, which contain all the events who kinematics fall within that range, with the backgrounds stacked on top of each. Each bin therefore predicts how many events will fall within that range and summing over all bins will predict how many events in total we expect to produce. Each plot contains the same number of events, as we are just looking at the same events but as a function of a different kinematic distribution. The small red histogram is our prediction of how many signal events we expect to produce and what they would like look (assuming the Standard Model is correct in its predictions). \n",
    "\n",
    "- Why is the signal drawn twice on the histograms?\n",
    "\n",
    "The solid red histogram shows the number of predicted Higgs boson events we expect to create based upon the Standard Model prediction, however, as it is stacked on top of all the other backgrounds it is difficult to see the shape. As such the distribution (typically scaled up by a factor of 20), is also drawn individually (that is, not stacked on top of the other backgrounds) as a solid red line, so you can clearer compare the shape differences between the signal and background.\n",
    "\n",
    "- What is a reasonable sensitivity for the cut-based and NN analysis?\n",
    "\n",
    "A reasonable sensitivity is around ~1.8 and a reasonable improvement over the cut-based analysis is ~20-30%.\n",
    "\n",
    "- Which type of neutral network should I use for the classifier?\n",
    "\n",
    "It is entirely up to you what type and configuration of NN you should use, but you should carefully think about which type of NN is most appropriate (dense, CNN, LSTM etc.) based upon your experience so far.\n",
    "\n",
    "- How should I choose the optimal parameters/configuration for the NN?\n",
    "\n",
    "Again, this is entirely up to you, but you should be able to provide a justification for the values you selected and to demonstrate your insight by explaining why this is a reasonable selection. In many such analyses, plots are made which shows how the key metrics under consideration vary as a function of the parameter being varied (for instance the number of nodes in a layer). Given there is typically some interplay between variables you will be optimising, some assumptions/simplifications usually have to be made in terms of how you optimise the parameters.\n",
    "\n",
    "- Why do we use the di-jet invariant mass (mBB) for the discriminant in the cut-based analysis and he NN output/score for the NN result?\n",
    "\n",
    "This is because we want to use the discriminant (that is the variable that is used to determine if we have seen a new particle), which provides the best separation between the signal and background, as that provides the best chance of observing a new particle. It can easily be shown that the best discriminant from the standard variables provided is mBB, but once the NN has been trained, that provides the best discrimination between the signal/background. The sensitivity (roughly $\\frac{S}{\\sqrt{B}}$, where S is the number of signal events and B is the number of background events) is calculated on a bin-by-bin basis and then added in quadrature $(\\sqrt{\\sum^N\\frac{S}{\\sqrt{B}}})$ to get the total sensitivity, so we want to calculate this on the distribution which has the best discrimination.\n",
    "\n",
    "- Can I use the other variables I’ve found in the raw data file as inputs to the NN?\n",
    "\n",
    "The variables not included in the list above should not be used, as they either provide no information (they are designed for a different event selection where we select 3-jets) or more dangerously, they provide information based upon the truth label, which if used would provide a biased input that would provide a greatly enhanced separation that isn’t real.\n",
    "\n",
    "- What is the 'EventWeight' variable in the file?\n",
    "\n",
    "To currently predict the number of events we expect to produce in the LHC for each of the background/signal processes, we weight each of the simulated events to match the predicted number of events (Number predicted = Luminosity x cross section). So along with a few data/MC correction factors, each event gets a weight ((Luminosity x cross section)/number of events generated), that when used to fill a histogram, ensures we accurately predict the number of events expected (rather than just the number we generated). As this weight is inherently linked to the sample we are generating (the signal is very rare and may have low weights, whereas backgrounds may have large weights), it contains information that is derived based upon the truth label, which could then be exploited to predict if it is signal or background (for instance all events with low weights are signal and all events with high values are background for instance) and this has no counterpart in real data, so could never be used to predict if a data event was signal/background. As such, you should not use this, or the closely related 'training_weight' or 'post_fit_weight' variables, as input variables in the training as it provides a way for the NN to cheat in predicting if it is signal or background (similar to if you just feed in the label as an input). We do however use these weights to more accurately predict the relative background composition when training, as discussed in the next FAQ.\n",
    "\n",
    "- What is the 'training_weight' option used when training the NN?\n",
    "\n",
    "When training, the 'training_weight' variable (which is calculated from the 'EventWeight' variable described in the previous question) is passed to Keras to be used as a 'sample_weight'. This means that during the training, each event has a weight associated to it (the 'training_weight') rather than having a weight of one, which ensures that the relative composition of the backgrounds used in the training accurately reflects the composition we expect in our data (else the relative composition of each of the backgrounds will be determined by how many events of each sample we generated, rather than our predictions of how many would be created in the pp collisions). If different events have different weights, then it means they have a larger (or smaller) impact in the loss function and will have a greater (or smaller) importance in the optimisation of the tunable parameters, therefore ensuring the correct relative level of importance is given to each of the backgrounds when training.\n",
    "\n",
    "- Can we use the functions provided for instance to calculate the sensitivity and plot the figures?\n",
    "\n",
    "As in the other exercises, all the functions provided are for you to use in your solutions and it is expected that you will use the provided functions rather than trying to reinvent them (especially given they carry out non-trivial functionality which is beyond the scope of your project).\n",
    "\n",
    "- How is the plotted NN decision value binning calculated and how does this relate to the raw decision value?\n",
    "\n",
    "The raw NN decision value assigned to each event is a value from 0 to 1 (with 1 more signal-like and 0 more backgroud-like) and you can see what this variable looks like by executing a command to plot it afer it has been stored in the data frames ('plot_variable(df,'decision_value')'. If you are trying to select events with a high NN decicion value to investigate what the input variables look like for such events, you will need to look at the raw decision_value to judge which value to cut on (you can then follow a procedure similar to that in the cut based analysis to select events with only a high decision_value and then draw the input variables for the remaining collisions). However, in terms of calculating the sensitivity we use a 'transformed' output, which exactly replicates what happens in the ATLAS analysis (and also allows fairer comparisons between all results). In the ATLAS results, the raw decision value output by the NN is 'transformed', by starting from the highest decision value events and grouping them all into one bin until the statistical uncertainty on the number of simulated events < 20% (the statistical framework used to extract the results has issues if this uncertainty is to large, plus such a large systematic uncertainty would really hurt the analysis sensitivity) and to keep repeating this procedure until all the events have been combined into bins. This produces the figure at the bottom of the NN notebook and is the only one you should use to calculate the sensitivity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based upon material originally produced by hackingEducation for use in outreach**  \n",
    "<img src=\"images/logo-black.png\" width=\"50\" align = 'left'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4f39047f4e9cc9b3ad58404b253244e49dbf604894e0d0c8162e12c995ace6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
